# Filcryo architecture and deployment docs

Filcryo is mostly a Lotus node bundled with some bash scripts into a Docker container.

* The Lotus version we run makes use of optimized [chain-ranged export](https://github.com/filecoin-project/lotus/pull/10145), to be upstreamed.
* Assumes and hardcodes a Google Cloud storage bucket (`fil-mainnet-archival-snapshots`).
* The default entrypoint [`scripts/entrypoint.sh`](scripts/entrypoint.sh) will:
  * Unless `/root/.lotus`, already present, initialize lotus from the latest archival snapshot in gcloud storage.
  * Wait until Lotus head catches up.
  * Wait until the time it can export the next snapshot (2880 epochs + 900 finality-epochs + 15).
  * Export, compress, upload to bucket
  * Repeat (any errors will retrigger this cycle too).
* While the wait-export loop is ongoing, the export some promethus metrics into files (`/root/metrics/metrics.prom`): current lotus height, latest snapshot epoch etc.

The bash scripts and functions used are part of [`filcryo.sh`](scripts/filcryo.sh). A container can potentially be started with a custom entrypoint and the functions re-used for manual archival tasks (running parallel snapshots etc).

Filcryo expects to run in "host" mode in a machine in GCP which has the right access scopes to the `fil-mainnet-archival-snapshots` storage. There is no configuration to this regard, and it expects to work out of the box (`gcloud` and `gsutil` are used).

## Operation in production

Running in production consists of running a docker-compose stack that:

* Runs the filcryo docker container
* Runs the grafana-agent container

Both run in "host" mode and have the right mounts configured (see [`docker-compose.yml`](docker-compose.yml)) so that metrics and logs generated by filcryo can be read and uploaded by grafana-agent, as well as making sure that the lotus repository survives restarts.

Grafana Agent is configured with [`config.yaml`](grafana-agent/config.yaml). The configuration makes it:
  * Launch node_exporter (grabs all host machine metrics and filcryo metrics). Prometheus `job` value for all metrics collected is `filcryo`.
  * Collect all docker containers logs: logs are tagged with `project:filcryo`. Additional tags include: `app` (filcryo/grafana-agent/lotus...
  * Upload everything to Grafana Cloud (urls hardcoded)

**Note that Grafana Agent picks up the configuration directly from Github main-branch (see `-config.file` flag in `docker-compose.yml`)**. This avoids having to pack up or provide the configuration on the side (and then mount it when running the container).

A `Filcryo` Grafana dashboard exists and tracks all the relevant stuff.

![image](https://user-images.githubusercontent.com/1027022/214831509-eca73672-79ad-42f3-b30c-71ebad00148a.png)


## Deployment

Infrastructure deployment is performed manually via Terraform (`terraform/` folder):

* The Terraform version is configured via asdf (see [.tool_versions](.tool_versions)).
* The state is stored in a remote Google Cloud storage bucket.
* Terraform creates the instance, roles, permissions, etc.
* The stack can be destroyed and re-created, but note that upon creation the initial snapshot download, import and sync takes a while.
* Terraform output shows the instance IP on completion.

The instance is configured with a [boot script](terraform/boot.sh) which:

* Installs the latest version of docker
* Clones this repository into `/opt/filcryo`
* Populates an `.env` file with the necessary secrets that have been previously stored in Gcloud Secrets Manager
* Installs an [`update_stack.sh`](scripts/update_stack.sh) cron-job that runs every minute.

The `update_stack.sh` script, in turn:

* Checks if branch `main` of this repository has new updates
* When the repository had updates it will:
  * Pull them
  * Rebuild the filcryo docker container locally
  * Redeploy the docker-compose stack

That means: **Filcryo is always running with the latest version of "main" and will automatically upgrade on push to "main"**. Caveats:

* The filcryo container is always rebuilt and restarted even when no changes affected the container
* The grafana-agent container is not restarted unless the version changed (changes to grafana-agent config require manual restart)
* A change in the `update_stack.sh` script is only effective on the second commit after it (because it is already running as it pulls the commit on which it changes itself).

System and application logs are pushed to Grafana/Loki (tagged with `project=filcryo`). The `update_stack.sh` and Lotus logs can be found at `/root/logs` on the host, additionally. Mostly everything happens at `/root`, where the working scripts downloaded and ongoing snapshots are placed by the Filcryo container.

## Access

The instance can be accessed via SSH. SSH-access is auto-magically handled by Google Cloud Compute (users configured there get they're usernames/keys automatically on the instance).

## Issue and alerts

The operation of Filcryo should be most maintenance free, but deployments and restarts are better done when no ongoing export or initialization is happening. There will be some common issues:

* Disk-full: the Lotus repository filled the disk: the best solution be to run `terraform destroy` and `terraform apply` and start on a fresh machine. This will need a few hours to sync and start doing snapshots again.
* Last snapshot happened long ago: something must have broken the process. Must check if lotus is running and what are the error logs when starting the export: are we waiting for Lotus to reach a certain height? Does the export give an error? Is Lotus alive or has it OOM'ed ? In general it should be fine to just `docker restart filcryo-filcryo-1` or `cd /opt/filcryo; docker compose down; docker compose up -d`.

Some alerts are configured in the Grafana dashboard and notify relevant slack channels when snapshots stopped to happen. Alerts should be tended to, but are not meant to trigger on-call events.

Right now it takes about 50 minutes to export an snapshot (+ 10 minutes for compression and 10 minutes for upload). These numbers will grow. They grow as the Lotus repository has more data, but in general they will block as the chain grows.
